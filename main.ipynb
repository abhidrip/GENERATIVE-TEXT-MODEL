{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aff9344",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# ================================================\n",
    "# CELL 1: Import dependencies\n",
    "# ================================================\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# Check if CUDA (GPU) is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6369bc49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e059ad63e0b45e8af534a2fd7aaa536",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Abhideep\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Abhideep\\.cache\\huggingface\\hub\\models--gpt2-medium. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1dcf5b7c340d4bd6a5425b3cc5bfe2eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/718 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e60954903fd404ba8318ef25f16ffe0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "051851389df6488d8b1006a9093314a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3cb60ee24ce24e58b5e730f984341617",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ce751af0d3649768f409eedb6674c22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.52G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f7af017927844968938c50a284a40a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ================================================\n",
    "# CELL 2 (Updated): Load GPT-2 Medium & generation function\n",
    "# ================================================\n",
    "\n",
    "# Load the GPT-2 Medium tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2-medium\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"gpt2-medium\").to(device)\n",
    "\n",
    "# Function to generate a paragraph based on a prompt\n",
    "def generate_paragraph(prompt, max_length=200):\n",
    "    # Encode the input prompt\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors='pt').to(device)\n",
    "    \n",
    "    # Generate output sequence\n",
    "    output = model.generate(\n",
    "        input_ids,\n",
    "        max_length=max_length,\n",
    "        num_return_sequences=1,\n",
    "        no_repeat_ngram_size=2,\n",
    "        temperature=0.7,     # slightly lower temp for factual text\n",
    "        top_p=0.9,\n",
    "        do_sample=True,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "    \n",
    "    # Decode and return the generated text\n",
    "    generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    return generated_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3264ba68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "ðŸ“„  GENERATED PARAGRAPH\n",
      "==================================================\n",
      "The first step is to write an artificial intelligence that can understand English. This is done by building a machine that understands English, and then building more machines that understand more English and so on. As the number of languages increases, the machine will get smarter.\n",
      " (Source: The Artificial Intelligence of Language: Artificial intelligence as a fundamental problem in cognitive science, MIT Press, 1998)\n",
      "A second step to artificial intelligences is that they need to be able to speak and understand other languages. For example, if you want to build a computer that is capable of understanding the spoken languages of people around the world, you need a language that the computer can speak. You need languages that are easily understood by the artificial intellects. (Note: the language you use for your computer is a very important part of your artificial mind.)\n",
      "-\n",
      "In the case of Artificial Intelligent Machines (AI), the goal is not to understand the languages\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# ================================================\n",
    "# CELL 3 (Updated): Interactive prompt with better paragraph generation\n",
    "# ================================================\n",
    "\n",
    "# Ask user for a topic\n",
    "user_topic = input(\"Enter a topic for the paragraph:\\n> \")\n",
    "\n",
    "# Use a clear instruction-style prompt for GPT-2 Medium\n",
    "better_prompt = f\"Write a detailed and coherent paragraph about {user_topic}:\\n\"\n",
    "\n",
    "# Generate a paragraph\n",
    "generated_paragraph = generate_paragraph(better_prompt, max_length=200)\n",
    "\n",
    "# Remove the prompt if repeated in output\n",
    "cleaned_paragraph = generated_paragraph.replace(better_prompt, \"\").strip()\n",
    "\n",
    "# Display clean result\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"ðŸ“„  GENERATED PARAGRAPH\")\n",
    "print(\"=\"*50)\n",
    "print(cleaned_paragraph)\n",
    "print(\"=\"*50)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
